{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本次研讨课内容:\n",
    "   问答摘要内容讲解:\n",
    "    1. vocab对象构建\n",
    "    2. batcher 方法构建\n",
    "    3. PGN Model.\n",
    "    4. coverage\n",
    "    5. coverage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/Text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "蓝色的字体表示的是参考摘要，三个模型的生成摘要的结果差别挺大。红色字体表明了不准确的摘要细节生成(UNK未登录词，无法解决OOV问题)，绿色的字体表明了模型生成了重复文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rouge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/aistudio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.PGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sequence-to-sequence mode baseline,\n",
    "2. pointer generater mode \n",
    "3. coverage机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "![](img/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGN\n",
    "![](img/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. vocab对象构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    PAD_TOKEN = '<PAD>'\n",
    "    UNKNOWN_TOKEN = '<UNK>'\n",
    "    START_DECODING = '<START>'\n",
    "    STOP_DECODING = '<STOP>'\n",
    "\n",
    "    def __init__(self, vocab_file, vocab_max_size=None):\n",
    "        \"\"\"\n",
    "        Vocab 对象,vocab基本操作封装\n",
    "        :param vocab_file: Vocab 存储路径\n",
    "        :param vocab_max_size: 最大字典数量\n",
    "        \"\"\"\n",
    "        self.word2id, self.id2word = self.load_vocab(vocab_file, vocab_max_size)\n",
    "        self.count = len(self.word2id)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(file_path, vocab_max_size=None):\n",
    "        \"\"\"\n",
    "        读取字典\n",
    "        :param file_path: 文件路径\n",
    "        :return: 返回读取后的字典\n",
    "        \"\"\"\n",
    "        vocab = {}\n",
    "        reverse_vocab = {}\n",
    "        for line in open(file_path, \"r\", encoding='utf-8').readlines():\n",
    "            word, index = line.strip().split(\"\\t\")\n",
    "            index = int(index)\n",
    "            # 如果vocab 超过了指定大小\n",
    "            # 跳出循环 截断\n",
    "            if vocab_max_size and index > vocab_max_size:\n",
    "                print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (\n",
    "                    vocab_max_size, index))\n",
    "                break\n",
    "            vocab[word] = index\n",
    "            reverse_vocab[index] = word\n",
    "        return vocab, reverse_vocab\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        if word not in self.word2id:\n",
    "            return self.word2id[self.UNKNOWN_TOKEN]\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def id_to_word(self, word_id):\n",
    "        if word_id not in self.id2word:\n",
    "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "        return self.id2word[word_id]\n",
    "\n",
    "    def size(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 1. oov词去哪里取?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. batcher 改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions -> idx2word -> `QQ224,这是车辆<UNK><UNK>`\n",
    "\n",
    "### enc_extended_inp -> idx2word ->  `道奇,锋哲,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "### enc_inp -> idx2word->  `<UNK>,<UNK>,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "### article_oovs ->` [道奇 , 锋哲]`\n",
    "\n",
    "### dec_input -> idx2word-> `<start>QQ224,这是车辆<UNK><UNK><end><pad><pad>`\n",
    "\n",
    "### target -> idx2word-> ` QQ224,这是车辆道奇,锋哲<end><pad>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(params, vocab, max_enc_len, max_dec_len, mode, batch_size),\n",
    "        output_types={\n",
    "            \"enc_len\": tf.int32,\n",
    "            \"enc_input\": tf.int32,\n",
    "            \"enc_extended_inp\": tf.int32,\n",
    "            \n",
    "            \"article_oovs\": tf.string,\n",
    "            \"dec_input\": tf.int32,\n",
    "            \"target\": tf.int32,\n",
    "            \"dec_len\": tf.int32,\n",
    "            \"article\": tf.string,\n",
    "            \"abstract\": tf.string,\n",
    "            \"abstract_sents\": tf.string,\n",
    "            \"sample_decoder_pad_mask\": tf.int32,\n",
    "            \"sample_encoder_pad_mask\": tf.int32,\n",
    "        },\n",
    "        output_shapes={\n",
    "            \"enc_len\": [],\n",
    "            \"enc_input\": [None],\n",
    "            \"enc_extended_inp\": [None],\n",
    "            \n",
    "            \"article_oovs\": [None],\n",
    "            \"dec_input\": [None],\n",
    "            \"target\": [None],\n",
    "            \"dec_len\": [],\n",
    "            \"article\": [],\n",
    "            \"abstract\": [],\n",
    "            \"abstract_sents\": [],\n",
    "            \"sample_decoder_pad_mask\": [None],\n",
    "            \"sample_encoder_pad_mask\": [None]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(batch_size,\n",
    "               padded_shapes=({\"enc_len\": [],\n",
    "                               \"enc_input\": [None],\n",
    "                               \"enc_input_extend_vocab\": [None],\n",
    "                               \"article_oovs\": [None],\n",
    "                               \"dec_input\": [max_dec_len],\n",
    "                               \"target\": [max_dec_len],\n",
    "                               \"dec_len\": [],\n",
    "                               \"article\": [],\n",
    "                               \"abstract\": [],\n",
    "                               }),\n",
    "               padding_values={\"enc_len\": -1,\n",
    "                               \"enc_input\": vocab.word2id[Vocab.PAD_TOKEN],\n",
    "                               \"enc_input_extend_vocab\": vocab.word2id[Vocab.PAD_TOKEN],\n",
    "                               \"article_oovs\": b'',\n",
    "                               \"dec_input\": vocab.word2id[Vocab.PAD_TOKEN],\n",
    "                               \"target\": vocab.word2id[Vocab.PAD_TOKEN],\n",
    "                               \"dec_len\": -1,\n",
    "                               \"article\": b\"\",\n",
    "                               \"abstract\": b\"\",\n",
    "                               },\n",
    "               drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ppgn  \n",
    "\n",
    "### seq2seq+point\n",
    "\n",
    "混合了 Baseline seq2seq和PointerNetwork的网络，它具有Baseline seq2seq的生成能力和PointerNetwork的Copy能力。如何权衡一个词应该是生成的还是复制的？原文中引入了一个权重$p_{gen}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从Baseline seq2seq的模型结构中得到了$S_t$和$h^*_t$，和解码器输入 $x_t$ 一起来计算 $p_{gen}$ ： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](readme/pgn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pgen ∈ [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ context vector $h^*_t$\n",
    "+ decoder input $x_t$ \n",
    "+ the decoder state $S_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.w_s_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_i_reduce = tf.keras.layers.Dense(1)\n",
    "        self.w_c_reduce = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, context_vector, dec_hidden, dec_inp):\n",
    "        return tf.nn.sigmoid(self.w_s_reduce(dec_hidden) + self.w_c_reduce(context_vector) + self.w_i_reduce(dec_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gens = []\n",
    "for t in range(dec_target.shape[1]):\n",
    "    .....\n",
    "    p_gen = self.pointer(context_vector, dec_hidden, dec_x)\n",
    "\n",
    "p_gens.append(p_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](readme/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时，会扩充单词表形成一个更大的单词表--扩充单词表(将原文当中的单词也加入到其中)，该时间步的预测词概率为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](readme/pw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $a_i^t$ 表示的是原文档中的词。我们可以看到解码器一个词的输出概率有其是否拷贝是否生成的概率和决定。当一个词不出现在常规的单词表上时$P_{vocab}(w)$ 为0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](readme/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predictions -> idx2word -> `QQ224,这是车辆<UNK><UNK>`\n",
    "\n",
    "## enc_extended_inp -> idx2word ->  `道奇,锋哲,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "## enc_inp -> idx2word->  `<UNK>,<UNK>,昨晚看到一台车车牌是绿色的？这是什么牌？`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dists=calc_final_dist(enc_extended_inp,# 原始输入\n",
    "                             predictions,# 原始预测概率\n",
    "                             attentions, # att权重\n",
    "                             p_gens, # pgn概率\n",
    "                             batch_oov_len,# 2\n",
    "                             self.params[\"vocab_size\"],# 原始的wocab size\n",
    "                             self.params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab -> `{\n",
    "    <UNK>:0,\n",
    "    昨晚:1,\n",
    "    看到:2,\n",
    "    一台车:3,\n",
    "    车牌:4,\n",
    "    是绿色的:5,\n",
    "    ？:6,\n",
    "    这是什么:7,\n",
    "    牌:8\n",
    "}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extended_vsize=10\n",
    "\n",
    "extend_vocab ->\n",
    "`{\n",
    "    <UNK>:0,\n",
    "    昨晚:1,\n",
    "    看到:2,\n",
    "    一台车:3,\n",
    "    车牌:4,\n",
    "    是绿色的:5,\n",
    "    ？:6,\n",
    "    这是什么:7,\n",
    "    牌:8,\n",
    "    道奇:9,\n",
    "    锋哲:10.\n",
    "}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions -> idx2word -> `QQ224,这是车辆<UNK><UNK>`\n",
    "\n",
    "### enc_extended_inp -> idx2word ->  `道奇,锋哲,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "### enc_inp -> idx2word->  `<UNK>,<UNK>,昨晚看到一台车车牌是绿色的？这是什么牌？`\n",
    "\n",
    "### article_oovs ->` [道奇 , 锋哲]`\n",
    "\n",
    "### dec_input -> idx2word-> `<start>QQ224,这是车辆<UNK><UNK><end><pad><pad>`\n",
    "\n",
    "### target -> idx2word-> ` QQ224,这是车辆道奇,锋哲<end><pad>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](readme/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc_inp <unk> <unk> 1 2 3 4 5 6 7 8\n",
    "\n",
    "enc_extended_inp 9 10 1 2 3 4 5 6 7 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dists=calc_final_dist(enc_extended_inp,# 原始输入\n",
    "                             predictions,# 原始预测概率\n",
    "                             attentions, # att权重\n",
    "                             p_gens, # pgn概率\n",
    "                             batch_oov_len,# 2\n",
    "                             self.params[\"vocab_size\"],# 原始的wocab size\n",
    "                             self.params[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](readme/pw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_final_dist(_enc_batch_extend_vocab, vocab_dists, attn_dists, p_gens, batch_oov_len, vocab_size, batch_size):\n",
    "    \"\"\"\n",
    "    Calculate the final distribution, for the pointer-generator model\n",
    "    Args:\n",
    "    vocab_dists: The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays.\n",
    "                The words are in the order they appear in the vocabulary file.\n",
    "    attn_dists: The attention distributions. List length max_dec_steps of (batch_size, attn_len) arrays\n",
    "    Returns:\n",
    "    final_dists: The final distributions. List length max_dec_steps of (batch_size, extended_vsize) arrays.\n",
    "    \"\"\"\n",
    "    # Multiply vocab dists by p_gen and attention dists by (1-p_gen)\n",
    "    vocab_dists = [p_gen * dist for (p_gen, dist) in zip(p_gens, vocab_dists)]\n",
    "    \n",
    "    attn_dists = [(1 - p_gen) * dist for (p_gen, dist) in zip(p_gens, attn_dists)]\n",
    "\n",
    "    # Concatenate some zeros to each vocabulary dist, to hold the probabilities for in-article OOV words\n",
    "    extended_vsize = vocab_size + batch_oov_len  # the maximum (over the batch) size of the extended vocabulary\n",
    "    extra_zeros = tf.zeros((batch_size, batch_oov_len))\n",
    "    # list length max_dec_steps of shape (batch_size, extended_vsize)\n",
    "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
    "\n",
    "    # Project the values in the attention distributions onto the appropriate entries in the final distributions\n",
    "    # This means that if a_i = 0.1 and the ith encoder word is w, and w has index 500 in the vocabulary,\n",
    "    # then we add 0.1 onto the 500th entry of the final distribution\n",
    "    # This is done for each decoder timestep.\n",
    "    # This is fiddly; we use tf.scatter_nd to do the projection\n",
    "    batch_nums = tf.range(0, limit=batch_size)  # shape (batch_size)\n",
    "    batch_nums = tf.expand_dims(batch_nums, 1)  # shape (batch_size, 1)\n",
    "    attn_len = tf.shape(_enc_batch_extend_vocab)[1]  # number of states we attend over\n",
    "    batch_nums = tf.tile(batch_nums, [1, attn_len])  # shape (batch_size, attn_len)\n",
    "    # shape (batch_size, enc_t, 2)\n",
    "    indices = tf.stack((batch_nums, _enc_batch_extend_vocab), axis=2)  \n",
    "    shape = [batch_size, extended_vsize]\n",
    "    \n",
    "    # list length max_dec_steps (batch_size, extended_vsize) extended_vsize = 30000 + 2\n",
    "    attn_dists_projected = [tf.scatter_nd(indices, copy_dist, shape) for copy_dist in attn_dists]\n",
    "\n",
    "    # Add the vocab distributions and the copy distributions together to get the final distributions\n",
    "    # final_dists is a list length max_dec_steps; each entry is a tensor shape (batch_size, extended_vsize) giving\n",
    "    # the final distribution for that decoder timestep\n",
    "    # Note that for decoder timesteps and examples corresponding to a [PAD] token, this is junk - ignore.\n",
    "    final_dists = [vocab_dist + copy_dist for (vocab_dist, copy_dist) in\n",
    "                   zip(vocab_dists_extended, attn_dists_projected)]\n",
    "\n",
    "    return final_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 .模型运行的时候 上一步预测出来的词 超出vocab范围,下一步输入会不会出问题?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using teacher forcing\n",
    "dec_input = tf.expand_dims(dec_target[:, t], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 替换掉 oov token unknown token\n",
    "latest_tokens = [t if t in vocab.id2word else unk_index for t in latest_tokens]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Seq2Seq和Point分别起到什么作用?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](readme/pgn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. attention\n",
    "2. loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output):\n",
    "        # query为上次的GRU隐藏层\n",
    "        # values为编码器的编码结果enc_output\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis)))\n",
    "       \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/e_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改造$e^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.W_c = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        # query 隐藏层\n",
    "        # values为 编码器的编码结果enc_output\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "        # self.W_s(values)  [batch_sz, max_len, units] self.W_h(hidden_with_time_axis) [batch_sz, 1, units]\n",
    "        # self.W_c(prev_coverage) [batch_sz, max_len, units]  score [batch_sz, max_len, 1]    \n",
    "        score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis) + self.W_c(prev_coverage)))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        # [batch_sz, max_len, enc_units]\n",
    "        context_vector = attention_weights * enc_output\n",
    "        # [batch_sz, enc_units]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask + coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W_s = tf.keras.layers.Dense(units)\n",
    "        self.W_h = tf.keras.layers.Dense(units)\n",
    "        self.W_c = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        # query为上次的GRU隐藏层\n",
    "        # values为编码器的编码结果enc_output\n",
    "        # 在seq2seq模型中，St是后面的query向量，而编码过程的隐藏状态hi是values。\n",
    "\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
    "\n",
    "        if use_coverage and prev_coverage is not None:\n",
    "            # self.W_s(values) [batch_sz, max_len, units] self.W_h(hidden_with_time_axis) [batch_sz, 1, units]\n",
    "            # self.W_c(prev_coverage) [batch_sz, max_len, units]  score [batch_sz, max_len, 1]\n",
    "            score = self.V(tf.nn.tanh(self.W_s(enc_output) + self.W_h(hidden_with_time_axis) + self.W_c(prev_coverage)))\n",
    "            # attention_weights shape (batch_size, max_len, 1)\n",
    "\n",
    "            mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "            masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "            masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "\n",
    "            attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "            coverage = attention_weights + prev_coverage\n",
    "        else:\n",
    "            # score shape == (batch_size, max_length, 1)\n",
    "            # we get 1 at the last axis because we are applying score to self.V\n",
    "            # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "            # 计算注意力权重值\n",
    "            score = self.V(tf.nn.tanh(\n",
    "                self.W_s(enc_output) + self.W_h(hidden_with_time_axis)))\n",
    "\n",
    "            mask = tf.cast(enc_pad_mask, dtype=score.dtype)\n",
    "            masked_score = tf.squeeze(score, axis=-1) * mask\n",
    "            masked_score = tf.expand_dims(masked_score, axis=2)\n",
    "\n",
    "            attention_weights = tf.nn.softmax(masked_score, axis=1)\n",
    "            # attention_weights = masked_attention(attention_weights)\n",
    "            if use_coverage:\n",
    "                coverage = attention_weights\n",
    "\n",
    "        # attention_weights sha== (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # # 使用注意力权重*编码器输出作为返回值，将来会作为解码器的输入\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector,attention_weights, coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 coverage_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/loss_t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def loss_function(real, pred):\n",
    "    pad_mask = tf.math.equal(real, pad_index)\n",
    "    mask = tf.math.logical_not(pad_mask)\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log loss + mask batch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def loss_function(real, pred, padding_mask):\n",
    "    loss = 0\n",
    "    for t in range(real.shape[1]):\n",
    "        if padding_mask:\n",
    "            loss_ = loss_object(real[:, t], pred[:, t, :])\n",
    "            mask = tf.cast(padding_mask[:, t], dtype=loss_.dtype)\n",
    "            loss_ *= mask\n",
    "            loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "            loss += loss_\n",
    "        else:\n",
    "            loss_ = loss_object(real[:, t], pred[:, t, :])\n",
    "            loss_ = tf.reduce_mean(loss_, axis=0)  # batch-wise\n",
    "            loss += loss_\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是将先前时间步的注意力权重加到一起得到所谓的覆盖向量 $c_t$ (coverage vector)，用先前的注意力权重决策来影响当前注意力权重的决策，这样就避免在同一位置重复，从而避免重复生成文本。计算上，先计算coverage vector $c_t$\n",
    "![](img/c_t.png)\n",
    "+ $c^t$就是一个长度为输入长度的向量\n",
    "+ 第一项是之前时刻输入第一个词attention权重的叠加和\n",
    "+ 加这个参数的目的是为了给attention之前生成词的信息，如果之前生成过这些词那么后续要抑制。抑制通过loss函数加惩罚项实现."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两个地方使用$c_t$:\n",
    "\n",
    "+ 注意力权重的计算过程中 $e^t_i$\n",
    "+ cov_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        pass\n",
    "        \n",
    "    def call(self, dec_hidden, enc_output, enc_pad_mask, use_coverage, prev_coverage):\n",
    "        if use_coverage and prev_coverage is not None:\n",
    "            pass\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            coverage = attention_weights + prev_coverage\n",
    "        else:    \n",
    "            if use_coverage:\n",
    "                coverage = attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/cobloss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<START> 举起 车辆 左 前轮 缸体 上 <STOP> <PAD> <PAD> `\n",
    "\n",
    "`padding_mask`->`[1,1,1,1,1,1,1,0,0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_coverage_loss(attn_dists, coverages, padding_mask):\n",
    "    \"\"\"\n",
    "    Calculates the coverage loss from the attention distributions.\n",
    "      Args:\n",
    "        attn_dists coverages: [max_len_y, batch_sz, max_len_x, 1]\n",
    "        padding_mask: shape (batch_size, max_len_y).\n",
    "      Returns:\n",
    "        coverage_loss: scalar\n",
    "    \"\"\"\n",
    "    cover_losses = []\n",
    "    # transfer attn_dists coverages to [max_len_y, batch_sz, max_len_x]\n",
    "    attn_dists = tf.squeeze(attn_dists, axis=3)\n",
    "    coverages = tf.squeeze(coverages, axis=3)\n",
    "\n",
    "    \n",
    "    for t in range(attn_dists.shape[0]):\n",
    "        cover_loss_ = tf.reduce_sum(tf.minimum(attn_dists[t, :, :], coverages[t, :, :]), axis=-1)  # max_len_x wise\n",
    "        cover_losses.append(cover_loss_)\n",
    "    \n",
    "    # change from[max_len_y, batch_sz] to [batch_sz, max_len_y]\n",
    "    cover_losses = tf.stack(cover_losses, 1)\n",
    "\n",
    "    # cover_loss_ [batch_sz, max_len_y]\n",
    "    mask = tf.cast(padding_mask, dtype=cover_loss_.dtype)\n",
    "    cover_losses *= mask\n",
    "    \n",
    "    # mean loss of each time step and then sum up\n",
    "    loss = tf.reduce_sum(tf.reduce_mean(cover_losses, axis=0))  \n",
    "    tf.print('coverage loss(batch sum):', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/loss_t_coverage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = loss_function(dec_target[:, 1:], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = loss_function(dec_target, predictions, padding_mask) + \\\n",
    "                         cov_loss_wt * coverage_loss(attentions, coverages, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义个coverage loss来多次惩罚对相对位置的关注.原理很直观，如果之前该词出现过了，那么它的$c^t_i$就很大，那么为了减少$loss$，就需要$a^t_i$变小（因为loss是取两者较小值）,$a^t_i$小就代表着这个位置被注意的概率减少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.constant 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=37, shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 1, 2],\n",
       "       [1, 1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1=tf.constant([[1,1,5],\n",
    "                [1,1,1]])\n",
    "\n",
    "x2=tf.constant([[1,3,2],\n",
    "                [3,1,3]])\n",
    "tf.minimum(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.reduce_sum 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=62, shape=(), dtype=int32, numpy=6>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.constant([[1,1,1],[1,1,1]])\n",
    "tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=64, shape=(3,), dtype=int32, numpy=array([2, 2, 2], dtype=int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=66, shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(x,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture02",
   "language": "python",
   "name": "lecture02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}